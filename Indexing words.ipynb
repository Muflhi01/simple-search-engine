{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.hadoop.conf.Configuration\n",
    "import org.apache.hadoop.io.{LongWritable, Text}\n",
    "import org.apache.hadoop.mapreduce.lib.input.TextInputFormat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@50280db1\n",
       "conf = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml\n",
       "dataset = file:///home/hrabo/Documents/skola/data-intensive-computing/project/data/CC-MAIN-20180918130631-20180918150631-00000.warc.wet NewHadoopRDD[75] at newAPIHadoopFile at <console>:174\n",
       "tempdata = MapPartitionsRDD[76] at map at <console>:183\n",
       "data = MapPartitionsRDD[77] at mapPartitionsWithIndex at <console>:185\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[77] at mapPartitionsWithIndex at <console>:185"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "                .builder\n",
    "                .master(\"local[*]\")\n",
    "                .appName(\"SparkSQL\")\n",
    "                .config(\"spark.cassandra.connection.host\", \"127.0.0.1\")\n",
    "                .getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "// Reading the file from disk\n",
    "val conf = new Configuration\n",
    "conf.set(\"textinputformat.record.delimiter\", \"WARC/1.0\")\n",
    "val dataset = spark\n",
    "                .sparkContext\n",
    "                .newAPIHadoopFile(\n",
    "                    \"file:///home/hrabo/Documents/skola/data-intensive-computing/project/data/CC-MAIN-20180918130631-20180918150631-00000.warc.wet\",\n",
    "//                     \"./data/WET/CC-MAIN-20180918130631-20180918150631-00000.warc.wet\",\n",
    "                    classOf[TextInputFormat], \n",
    "                    classOf[LongWritable], \n",
    "                    classOf[Text], \n",
    "                    conf\n",
    "                )\n",
    "\n",
    "val tempdata = dataset.map(x=>x._2.toString)      \n",
    "\n",
    "val data = tempdata.mapPartitionsWithIndex {  // Drop invalid records\n",
    "                    (idx, iter) => if (idx == 0) iter.drop(2) else iter \n",
    "                  }\n",
    "\n",
    "// data.take(1).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:160: error: value config is not a member of org.apache.spark.sql.SparkSession\n",
       "       spark.config\n",
       "             ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// data.take(1).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+\n",
      "|                 uri|topDomain|                text|\n",
      "+--------------------+---------+--------------------+\n",
      "|http://0-50.ru/ne...|       ru|Новости 0-50.ru |...|\n",
      "|http://000elham00...|      com|تبلیغات\t\n",
      "مسخره - ...|\n",
      "|http://0105335908...|       kr|맛집? 많은분 들이 이곳을 이용...|\n",
      "|http://01ninchisy...|      net|認知症ケアマニュアル: アルツハイ...|\n",
      "|http://01xxxx.com...|      com|正在播放 5-鬼畜老师强暴学生妹[...|\n",
      "|http://023pifu.co...|      com|皮肤病 / 荨麻疹_重庆皮肤病医院...|\n",
      "|http://03e98e7.ne...|      com|Amicus Brief FAQs...|\n",
      "|http://0411auto.c...|      com|2017款宝马X5 xDrive3...|\n",
      "|http://0412hitomi...|      com|こら！ぼたち！ ダッシュするときも...|\n",
      "|http://080ek21.co...|      com|網路聊天聯盟\n",
      "回 首 頁 │ 點數...|\n",
      "|http://09irk.ru/r...|       ru|Мангал | Леко мно...|\n",
      "|http://0ino0.beon...|       ru|Sasori в дневнике...|\n",
      "|http://0we.kunlun...|      com|Welcome 0we.kunlu...|\n",
      "|http://1-800-opti...|      com|Unisex Patches\n",
      "1-...|\n",
      "|http://1-tech.com...|       ua|Купить CABHD-360-...|\n",
      "|http://10-carat-d...|      com|Fascination About...|\n",
      "|http://1000awesom...|      com|#19 Remembering a...|\n",
      "|http://1001qfo.in...|     info|- На какой глубин...|\n",
      "|http://100news.bi...|      biz|Пораненого в Крам...|\n",
      "|http://101.ru/tra...|       ru|🎼 Слушать Lost i...|\n",
      "+--------------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "uriDataPairs = MapPartitionsRDD[79] at map at <console>:178\n",
       "pageInfo = MapPartitionsRDD[80] at map at <console>:192\n",
       "pageInfoDF = [uri: string, topDomain: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[uri: string, topDomain: string ... 1 more field]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// List of tuples containing (metadata, file_content)\n",
    "import java.net.URL\n",
    "\n",
    "val uriDataPairs = data.map(record => (record.split(\"\\r\\n\\r\\n\")))\n",
    "                       .map(dataItemAsList => {\n",
    "                            // Pairs of (Property, Value) from the meta data\n",
    "                            val keyValuePairs = dataItemAsList(0).split(\"\\r\\n\").filter(_ != \"\").map(line => {\n",
    "                                val splitList = line.split(\": \")\n",
    "                                ((splitList(0), splitList(1)))\n",
    "                            }) \n",
    "                            \n",
    "                            // Selecting the first (and only) record and the value from that\n",
    "                            val targetURL = keyValuePairs.filter(_._1 == \"WARC-Target-URI\")(0)._2\n",
    "                            \n",
    "                            ((targetURL,dataItemAsList(1)))\n",
    "                       })//.take(2)(1)._1\n",
    "\n",
    "// Create DF\n",
    "val pageInfo = uriDataPairs.map(item => {\n",
    "    val host = new URL(item._1).getHost\n",
    "    val domainParts = host.replaceAll(\"\"\"([^\\.]*+\\.)\"\"\", \"\") // Replace everything except top domain\n",
    "    (item._1, domainParts, item._2)\n",
    "})//.take(1)\n",
    "\n",
    "// val topDomainCom = pageInfo.filter(x)\n",
    "\n",
    "val pageInfoDF = pageInfo.toDF(\"uri\", \"topDomain\", \"text\")\n",
    "pageInfoDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+--------------------+\n",
      "|                 uri|topDomain|                text|            keywords|\n",
      "+--------------------+---------+--------------------+--------------------+\n",
      "|http://0-50.ru/ne...|       ru|Новости 0-50.ru |...|[[2, 13], [1:1, 4...|\n",
      "|http://000elham00...|      com|تبلیغات\t\n",
      "مسخره - ...|[[1396, 15], [139...|\n",
      "|http://0105335908...|       kr|맛집? 많은분 들이 이곳을 이용...|[[01053359081.kte...|\n",
      "|http://01ninchisy...|      net|認知症ケアマニュアル: アルツハイ...|[[(7, 2], [(5, 2]...|\n",
      "|http://01xxxx.com...|      com|正在播放 5-鬼畜老师强暴学生妹[...|[[2017-02-16, 13]...|\n",
      "|http://023pifu.co...|      com|皮肤病 / 荨麻疹_重庆皮肤病医院...|[[hospital, 3], [...|\n",
      "|http://03e98e7.ne...|      com|Amicus Brief FAQs...|[[appellate, 4], ...|\n",
      "|http://0411auto.c...|      com|2017款宝马X5 xDrive3...|[[xdrive35i, 3], ...|\n",
      "|http://0412hitomi...|      com|こら！ぼたち！ ダッシュするときも...|[[(31, 15], [(30,...|\n",
      "|http://080ek21.co...|      com|網路聊天聯盟\n",
      "回 首 頁 │ 點數...|[[ut, 40], [0401,...|\n",
      "|http://09irk.ru/r...|       ru|Мангал | Леко мно...|[[+7(3952, 3], [p...|\n",
      "|http://0ino0.beon...|       ru|Sasori в дневнике...|[[re, 7], [needle...|\n",
      "|http://0we.kunlun...|      com|Welcome 0we.kunlu...|[[us, 2], [web, 2...|\n",
      "|http://1-800-opti...|      com|Unisex Patches\n",
      "1-...|[[lens, 37], [pat...|\n",
      "|http://1-tech.com...|       ua|Купить CABHD-360-...|[[hdmi, 43], [dvi...|\n",
      "|http://10-carat-d...|      com|Fascination About...|[[dating, 7], [ad...|\n",
      "|http://1000awesom...|      com|#19 Remembering a...|[[first, 8], [awe...|\n",
      "|http://1001qfo.in...|     info|- На какой глубин...|[[0,03, 2], [1800...|\n",
      "|http://100news.bi...|      biz|Пораненого в Крам...|[[2015, 18], [201...|\n",
      "|http://101.ru/tra...|       ru|🎼 Слушать Lost i...|[[linkin, 24], [p...|\n",
      "+--------------------+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "extractKeyWords = UserDefinedFunction(<function1>,ArrayType(StructType(StructField(_1,StringType,true), StructField(_2,IntegerType,false)),true),Some(List(StringType)))\n",
       "keywordDF = [uri: string, topDomain: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[uri: string, topDomain: string ... 2 more fields]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._ \n",
    "\n",
    "val extractKeyWords = udf[Array[(String, Int)], String](input => {\n",
    "    val stopWords = Set(\"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"he\",\"him\",\"his\",\"himself\",\"she\",\"her\",\"hers\",\"herself\",\"it\",\"its\",\"itself\",\"they\",\"them\",\"their\",\"theirs\",\"themselves\",\"what\",\"which\",\"who\",\"whom\",\"this\",\"that\",\"these\",\"those\",\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\"because\",\"as\",\"until\",\"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\"into\",\"through\",\"during\",\"before\",\"after\",\"above\",\"below\",\"to\",\"from\",\"up\",\"down\",\"in\",\"out\",\"on\",\"off\",\"over\",\"under\",\"again\",\"further\",\"then\",\"once\",\"here\",\"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\"each\",\"few\",\"more\",\"most\",\"other\",\"some\",\"such\",\"no\",\"nor\",\"not\",\"only\",\"own\",\"same\",\"so\",\"than\",\"too\",\"very\",\"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"should\",\"now\")\n",
    "    \n",
    "    input\n",
    "      .split(\"[\\\\s|\\n]\")\n",
    "      .map(s => s.replaceAll(\"\"\"[^\\x00-\\x7F]\"\"\", \"\")) // We only manage ascii chars\n",
    "      .map(s => s.replaceAll(\"\"\"^\\p{Punct}*$\"\"\", \"\")) // No words with only punctuation\n",
    "      .map(s => s.replaceAll(\"\"\"\\p{Punct}$\"\"\", \"\"))   // No trailing punctuations\n",
    "      .filter(_ != \"\")                                // No empty words\n",
    "      .map(_.toLowerCase)\n",
    "      .filter(!stopWords.contains(_))                 // Remove stop-words\n",
    "      .groupBy(identity).mapValues(_.size).toArray\n",
    "      .sortBy(- _._2)\n",
    "})\n",
    "\n",
    "\n",
    "val keywordDF = pageInfoDF.withColumn(\n",
    "    \"keywords\",\n",
    "    extractKeyWords(col(\"text\"))\n",
    "  )\n",
    "\n",
    "keywordDF\n",
    "//     .select(\"keywords\")\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------------+\n",
      "|  keyword|keyword_weight|                 uri|\n",
      "+---------+--------------+--------------------+\n",
      "|        2|            13|http://0-50.ru/ne...|\n",
      "|      1:1|             4|http://0-50.ru/ne...|\n",
      "|       15|             3|http://0-50.ru/ne...|\n",
      "|      1:0|             3|http://0-50.ru/ne...|\n",
      "|      0:1|             3|http://0-50.ru/ne...|\n",
      "|      1:2|             3|http://0-50.ru/ne...|\n",
      "|     2017|             3|http://0-50.ru/ne...|\n",
      "|      2:0|             2|http://0-50.ru/ne...|\n",
      "|       22|             2|http://0-50.ru/ne...|\n",
      "|    -2114|             2|http://0-50.ru/ne...|\n",
      "|      0:2|             2|http://0-50.ru/ne...|\n",
      "|        5|             2|http://0-50.ru/ne...|\n",
      "|      2:1|             2|http://0-50.ru/ne...|\n",
      "|    -2012|             2|http://0-50.ru/ne...|\n",
      "|      0:0|             2|http://0-50.ru/ne...|\n",
      "|        7|             2|http://0-50.ru/ne...|\n",
      "|      rss|             2|http://0-50.ru/ne...|\n",
      "|     2014|             1|http://0-50.ru/ne...|\n",
      "|       45|             1|http://0-50.ru/ne...|\n",
      "|1@0-50.ru|             1|http://0-50.ru/ne...|\n",
      "+---------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "keywordsExpandedDF = [keyword: string, keyword_weight: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[keyword: string, keyword_weight: int ... 1 more field]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// create one list with all keywords\n",
    "// keywords distinct = 2017741 (.distinct.count)\n",
    "// keywords total = 15 000 000\n",
    "// total_nr_records = 39653\n",
    "val keywordsExpandedDF = keywordDF\n",
    "                            .select(\"keywords\", \"uri\")\n",
    "                            .withColumn(\"keywords\", explode(($\"keywords\")))\n",
    "                            .select(\"keywords._1\", \"keywords._2\", \"uri\")\n",
    "                            .withColumnRenamed(\"_1\", \"keyword\")\n",
    "                            .withColumnRenamed(\"_2\", \"keyword_weight\")\n",
    "keywordsExpandedDF.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reverseIndex = [keyword: string, uri_list: array<string> ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[keyword: string, uri_list: array<string> ... 1 more field]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var reverseIndex = keywordsExpandedDF\n",
    "                        .groupBy($\"keyword\")\n",
    "                        .agg(\n",
    "                            collect_list($\"uri\").alias(\"uri_list\"), \n",
    "                            collect_list($\"keyword_weight\").alias(\"keyword_weights\")\n",
    "                            ) \n",
    "                        // We can probably collect as struct collect_list(struct($\"uri\", $\"keyword_weight\")).as(\"set\")\n",
    "                        // This might however lead to problems: https://stackoverflow.com/questions/31864744/spark-dataframes-groupby-into-list\n",
    "                        \n",
    "// Showing here takes 1 minute\n",
    "// reducedKeywords.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:79: error: overloaded method value groupBy with alternatives:\n",
       "  (col1: String,cols: String*)org.apache.spark.sql.RelationalGroupedDataset <and>\n",
       "  (cols: org.apache.spark.sql.Column*)org.apache.spark.sql.RelationalGroupedDataset\n",
       " cannot be applied to (org.apache.spark.sql.ColumnName)\n",
       "                               .groupBy($\"keyword\")\n",
       "                                ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val hellos = reducedKeywords.filter($\"keyword\" === \"hello\").select($\"uri_list\").cache\n",
    "// hellos.show()\n",
    "\n",
    "// // Numer of pages using the word \"hello\" = 756\n",
    "// println(hellos.first.getList(0).size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.11/2.0.9/spark-cassandra-connector_2.11-2.0.9.jar\n",
      "Finished download of spark-cassandra-connector_2.11-2.0.9.jar\n",
      "Starting download from https://repo1.maven.org/maven2/com/datastax/cassandra/cassandra-driver-core/3.0.0/cassandra-driver-core-3.0.0.jar\n",
      "Finished download of cassandra-driver-core-3.0.0.jar\n"
     ]
    }
   ],
   "source": [
    "// Download cassandra dependencies\n",
    "%AddJar https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.11/2.0.9/spark-cassandra-connector_2.11-2.0.9.jar -f\n",
    "%AddJar https://repo1.maven.org/maven2/com/datastax/cassandra/cassandra-driver-core/3.0.0/cassandra-driver-core-3.0.0.jar -f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster = com.datastax.driver.core.Cluster@773970c9\n",
       "session = com.datastax.driver.core.SessionManager@70fa7c06\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "com.datastax.driver.core.SessionManager@70fa7c06"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Import and connect to cassandra\n",
    "import org.apache.spark.sql.cassandra._\n",
    "import com.datastax.spark.connector._\n",
    "import com.datastax.driver.core.{Session, Cluster, Host, Metadata}\n",
    "import com.datastax.spark.connector.cql.CassandraConnectorConf\n",
    "import com.datastax.spark.connector.rdd.ReadConf\n",
    "\n",
    "// Setting up for cassandra\n",
    "// spark.setCassandraConf(CassandraConnectorConf.KeepAliveMillisParam.option(10000))\n",
    "// spark.setCassandraConf(\"localCluster\", CassandraConnectorConf.ConnectionHostParam.option(\"127.0.0.1\")) \n",
    "// spark.setCassandraConf(\"localCluster\", \"search\", ReadConf.SplitSizeInMBParam.option(128))\n",
    "\n",
    "val cluster = Cluster.builder().addContactPoint(\"127.0.0.1\").build()\n",
    "val session = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: com.datastax.driver.core.exceptions.NoHostAvailableException\n",
       "Message: All host(s) tried for query failed (no host was tried)\n",
       "StackTrace:   at com.datastax.driver.core.exceptions.NoHostAvailableException.copy(NoHostAvailableException.java:84)\n",
       "  at com.datastax.driver.core.exceptions.NoHostAvailableException.copy(NoHostAvailableException.java:37)\n",
       "  at com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:37)\n",
       "  at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:245)\n",
       "  at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:68)\n",
       "  at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:43)\n",
       "  ... 88 elided\n",
       "Caused by: com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (no host was tried)\n",
       "  at com.datastax.driver.core.RequestHandler.reportNoMoreHosts(RequestHandler.java:211)\n",
       "  at com.datastax.driver.core.RequestHandler.access$1000(RequestHandler.java:46)\n",
       "  at com.datastax.driver.core.RequestHandler$SpeculativeExecution.findNextHostAndQuery(RequestHandler.java:275)\n",
       "  at com.datastax.driver.core.RequestHandler.startNewExecution(RequestHandler.java:115)\n",
       "  at com.datastax.driver.core.RequestHandler.sendRequest(RequestHandler.java:95)\n",
       "  at com.datastax.driver.core.SessionManager.executeAsync(SessionManager.java:132)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create tables\n",
    "session.execute(\"CREATE KEYSPACE IF NOT EXISTS search WITH REPLICATION =\" +\n",
    "                \"{'class': 'SimpleStrategy', 'replication_factor': 1};\")\n",
    "session.execute(\"CREATE TABLE IF NOT EXISTS search.keywords (keyword text PRIMARY KEY, links list<text>, occurences list<int>);\")\n",
    "\n",
    "\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[71] at rdd at <console>:126"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverseIndex.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: java.lang.NoClassDefFoundError\n",
       "Message: Could not initialize class org.apache.spark.sql.cassandra.DefaultSource$\n",
       "StackTrace:   at org.apache.spark.sql.cassandra.package$DataFrameWriterWrapper$.cassandraFormat$extension1(package.scala:55)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Saving to cassandra\n",
    "\n",
    "// connect to Cassandra and make a keyspace and table as explained in the document\n",
    "// reverseIndex.rdd.saveToCassandra(\"search\", \"keywords\",  SomeColumns(\"keyword\", \"links\", \"occurences\"))\n",
    "// reverseIndex.rdd.bulkSaveToCassandra(\"search\", \"keywords\")\n",
    "\n",
    "reverseIndex\n",
    "    .write\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\n",
    "    .cassandraFormat(\"keywords\", \"search\")\n",
    "//     .options(Map(\"keyspace\"->\"search\",\"table\"->\"keywords\"))\n",
    "//     .mode(SaveMode.Append)\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
