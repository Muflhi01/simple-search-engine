{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.hadoop.conf.Configuration\n",
    "import org.apache.hadoop.io.{LongWritable, Text}\n",
    "import org.apache.hadoop.mapreduce.lib.input.TextInputFormat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@762611da\n",
       "conf = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml\n",
       "dataset = file:///home/hrabo/Documents/skola/data-intensive-computing/project/data/WET/*.wet NewHadoopRDD[52] at newAPIHadoopFile at <console>:59\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "file:///home/hrabo/Documents/skola/data-intensive-computing/project/data/WET/*.wet NewHadoopRDD[52] at newAPIHadoopFile at <console>:59"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "                .builder\n",
    "                .master(\"local[*]\")\n",
    "                .appName(\"SparkSQL\")\n",
    "                .config(\"spark.cassandra.connection.host\", \"127.0.0.1\")\n",
    "                .getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "// Reading the file from disk\n",
    "val conf = new Configuration\n",
    "conf.set(\"textinputformat.record.delimiter\", \"WARC/1.0\")\n",
    "val dataset = spark\n",
    "                .sparkContext\n",
    "                .newAPIHadoopFile(\n",
    "                    \"file:///home/hrabo/Documents/skola/data-intensive-computing/project/data/WET/*.wet\",\n",
    "//                     \"../data/WET/CC-MAIN-20180918130631-20180918150631-00000.warc.wet\",\n",
    "                    classOf[TextInputFormat], \n",
    "                    classOf[LongWritable], \n",
    "                    classOf[Text], \n",
    "                    conf\n",
    "                )\n",
    "\n",
    "import org.apache.spark.sql.functions._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- header: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+\n",
      "|                text|                 uri|              domain|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|Новости 0-50.ru |...|http://0-50.ru/ne...|             0-50.ru|\n",
      "|تبلیغات\t\n",
      "مسخره - ...|http://000elham00...|000elham00.mihanb...|\n",
      "|맛집? 많은분 들이 이곳을 이용...|http://0105335908...|01053359081.kte11...|\n",
      "|認知症ケアマニュアル: アルツハイ...|http://01ninchisy...|01ninchisyou.sees...|\n",
      "|正在播放 5-鬼畜老师强暴学生妹[...|http://01xxxx.com...|          01xxxx.com|\n",
      "|皮肤病 / 荨麻疹_重庆皮肤病医院...|http://023pifu.co...|         023pifu.com|\n",
      "|Amicus Brief FAQs...|http://03e98e7.ne...|03e98e7.netsolhos...|\n",
      "|2017款宝马X5 xDrive3...|http://0411auto.c...|        0411auto.com|\n",
      "|こら！ぼたち！ ダッシュするときも...|http://0412hitomi...|0412hitomi.blog.f...|\n",
      "|網路聊天聯盟\n",
      "回 首 頁 │ 點數...|http://080ek21.co...|         080ek21.com|\n",
      "|Мангал | Леко мно...|http://09irk.ru/r...|            09irk.ru|\n",
      "|Sasori в дневнике...|http://0ino0.beon...|       0ino0.beon.ru|\n",
      "|Welcome 0we.kunlu...|http://0we.kunlun...|    0we.kunlunjd.com|\n",
      "|Unisex Patches\n",
      "1-...|http://1-800-opti...|1-800-optisource.com|\n",
      "|Купить CABHD-360-...|http://1-tech.com...|       1-tech.com.ua|\n",
      "|Fascination About...|http://10-carat-d...|10-carat-diamond-...|\n",
      "|#19 Remembering a...|http://1000awesom...|1000awesomethings...|\n",
      "|- На какой глубин...|http://1001qfo.in...|        1001qfo.info|\n",
      "|Пораненого в Крам...|http://100news.bi...|         100news.biz|\n",
      "|🎼 Слушать Lost i...|http://101.ru/tra...|              101.ru|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tempdata = [text: string, uri: string ... 1 more field]\n",
       "tempdata = [text: string, uri: string ... 1 more field]\n",
       "tempdata = [text: string, uri: string ... 1 more field]\n",
       "tempdata = [text: string, uri: string ... 1 more field]\n",
       "tempdata = [text: string, uri: string ... 1 more field]\n",
       "tempdata = [text: string, uri: string ... 1 more field]\n",
       "regexpr = [^((https?):\\/\\/)]((www|www1)\\.)?([\\w-\\.]+)\n",
       "tempdata = [text: string, uri: string ... 1 more field]\n",
       "pageInfoDF = [text: string, uri: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[text: string, uri: string ... 1 more field]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var tempdata = dataset.map(x=>x._2.toString).toDF();\n",
    "\n",
    "// Filter empty stuff\n",
    "tempdata = tempdata.filter(length($\"value\") > 0)\n",
    "\n",
    "// Split header\n",
    "tempdata = tempdata.withColumn(\"header\", \n",
    "                        split($\"value\", \"\\r\\n\\r\\n\").getItem(0)\n",
    "                    )\n",
    "                    .withColumn(\"text\", \n",
    "                        split($\"value\", \"\\r\\n\\r\\n\").getItem(1)\n",
    "                    )\n",
    "                    .drop(\"value\")\n",
    "\n",
    "// preparse header\n",
    "tempdata = tempdata.withColumn(\"header\",\n",
    "                        split($\"header\", \"\\r\\n\")\n",
    "                    )\n",
    "//                     .withColum(\"header\",\n",
    "//                         filter($\"header\")\n",
    "//                     )\n",
    "\n",
    "tempdata.printSchema()\n",
    "\n",
    "tempdata = tempdata.withColumn(\"warc-type\",\n",
    "                    ($\"header\").getItem(1)\n",
    "                )\n",
    "                .filter($\"warc-type\" === \"WARC-Type: conversion\")\n",
    "                .drop(\"warc-type\")\n",
    "\n",
    "\n",
    "// Extract uri\n",
    "tempdata = tempdata.withColumn(\"uri\",\n",
    "                        ($\"header\").getItem(2)\n",
    "                    )\n",
    "                    .withColumn(\"uri\",\n",
    "                        split($\"uri\", \": \").getItem(1)\n",
    "                    )\n",
    "                    .drop(\"header\")\n",
    "\n",
    "// Extract domain\n",
    "val regexpr = \"\"\"[^((https?):\\/\\/)]((www|www1)\\.)?([\\w-\\.]+)\"\"\"\n",
    "tempdata = tempdata.withColumn(\"domain\",\n",
    "                    regexp_extract($\"uri\", regexpr, 0)\n",
    "                )\n",
    "                \n",
    "// println(warctype.first.getString(0))\n",
    "// tempdata.show();\n",
    "\n",
    "val pageInfoDF = tempdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|                 uri|              domain|            keywords|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|Новости 0-50.ru |...|http://0-50.ru/ne...|             0-50.ru|[[2, 13], [1:1, 4...|\n",
      "|تبلیغات\t\n",
      "مسخره - ...|http://000elham00...|000elham00.mihanb...|[[1396, 15], [139...|\n",
      "|맛집? 많은분 들이 이곳을 이용...|http://0105335908...|01053359081.kte11...|[[01053359081.kte...|\n",
      "|認知症ケアマニュアル: アルツハイ...|http://01ninchisy...|01ninchisyou.sees...|[[(7, 2], [(5, 2]...|\n",
      "|正在播放 5-鬼畜老师强暴学生妹[...|http://01xxxx.com...|          01xxxx.com|[[2017-02-16, 13]...|\n",
      "|皮肤病 / 荨麻疹_重庆皮肤病医院...|http://023pifu.co...|         023pifu.com|[[hospital, 3], [...|\n",
      "|Amicus Brief FAQs...|http://03e98e7.ne...|03e98e7.netsolhos...|[[appellate, 4], ...|\n",
      "|2017款宝马X5 xDrive3...|http://0411auto.c...|        0411auto.com|[[xdrive35i, 3], ...|\n",
      "|こら！ぼたち！ ダッシュするときも...|http://0412hitomi...|0412hitomi.blog.f...|[[(31, 15], [(30,...|\n",
      "|網路聊天聯盟\n",
      "回 首 頁 │ 點數...|http://080ek21.co...|         080ek21.com|[[ut, 40], [0401,...|\n",
      "|Мангал | Леко мно...|http://09irk.ru/r...|            09irk.ru|[[+7(3952, 3], [p...|\n",
      "|Sasori в дневнике...|http://0ino0.beon...|       0ino0.beon.ru|[[re, 7], [needle...|\n",
      "|Welcome 0we.kunlu...|http://0we.kunlun...|    0we.kunlunjd.com|[[us, 2], [web, 2...|\n",
      "|Unisex Patches\n",
      "1-...|http://1-800-opti...|1-800-optisource.com|[[lens, 37], [pat...|\n",
      "|Купить CABHD-360-...|http://1-tech.com...|       1-tech.com.ua|[[hdmi, 43], [dvi...|\n",
      "|Fascination About...|http://10-carat-d...|10-carat-diamond-...|[[dating, 7], [ad...|\n",
      "|#19 Remembering a...|http://1000awesom...|1000awesomethings...|[[first, 8], [awe...|\n",
      "|- На какой глубин...|http://1001qfo.in...|        1001qfo.info|[[0,03, 2], [1800...|\n",
      "|Пораненого в Крам...|http://100news.bi...|         100news.biz|[[2015, 18], [201...|\n",
      "|🎼 Слушать Lost i...|http://101.ru/tra...|              101.ru|[[linkin, 24], [p...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "extractKeyWords = UserDefinedFunction(<function1>,ArrayType(StructType(StructField(_1,StringType,true), StructField(_2,IntegerType,false)),true),Some(List(StringType)))\n",
       "keywordDF = [text: string, uri: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[text: string, uri: string ... 2 more fields]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._ \n",
    "\n",
    "val extractKeyWords = udf[Array[(String, Int)], String](input => {\n",
    "    val stopWords = Set(\"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"he\",\"him\",\"his\",\"himself\",\"she\",\"her\",\"hers\",\"herself\",\"it\",\"its\",\"itself\",\"they\",\"them\",\"their\",\"theirs\",\"themselves\",\"what\",\"which\",\"who\",\"whom\",\"this\",\"that\",\"these\",\"those\",\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\"because\",\"as\",\"until\",\"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\"into\",\"through\",\"during\",\"before\",\"after\",\"above\",\"below\",\"to\",\"from\",\"up\",\"down\",\"in\",\"out\",\"on\",\"off\",\"over\",\"under\",\"again\",\"further\",\"then\",\"once\",\"here\",\"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\"each\",\"few\",\"more\",\"most\",\"other\",\"some\",\"such\",\"no\",\"nor\",\"not\",\"only\",\"own\",\"same\",\"so\",\"than\",\"too\",\"very\",\"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"should\",\"now\")\n",
    "    \n",
    "    input\n",
    "      .split(\"[\\\\s|\\n]\")\n",
    "      .map(s => s.replaceAll(\"\"\"[^\\x00-\\x7F]\"\"\", \"\")) // We only manage ascii chars\n",
    "      .map(s => s.replaceAll(\"\"\"^\\p{Punct}*$\"\"\", \"\")) // No words with only punctuation\n",
    "      .map(s => s.replaceAll(\"\"\"\\p{Punct}$\"\"\", \"\"))   // No trailing punctuations\n",
    "      .filter(_ != \"\")                                // No empty words\n",
    "      .map(_.toLowerCase)\n",
    "      .filter(!stopWords.contains(_))                 // Remove stop-words\n",
    "      .groupBy(identity).mapValues(_.size).toArray\n",
    "      .sortBy(- _._2)\n",
    "})\n",
    "\n",
    "\n",
    "val keywordDF = pageInfoDF.withColumn(\n",
    "    \"keywords\",\n",
    "    extractKeyWords(col(\"text\"))\n",
    "  )\n",
    "\n",
    "keywordDF\n",
    "//     .select(\"keywords\")\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+---------+--------------+\n",
      "|                text|                 uri| domain|  keyword|keyword_weight|\n",
      "+--------------------+--------------------+-------+---------+--------------+\n",
      "|Новости 0-50.ru |...|http://0-50.ru/ne...|0-50.ru|        2|            13|\n",
      "|Новости 0-50.ru |...|http://0-50.ru/ne...|0-50.ru|      1:1|             4|\n",
      "|Новости 0-50.ru |...|http://0-50.ru/ne...|0-50.ru|       15|             3|\n",
      "|Новости 0-50.ru |...|http://0-50.ru/ne...|0-50.ru|      1:0|             3|\n",
      "|Новости 0-50.ru |...|http://0-50.ru/ne...|0-50.ru|      0:1|             3|\n",
      "|Новости 0-50.ru |...|http://0-50.ru/ne...|0-50.ru|      1:2|             3|\n",
      "|Новости 0-50.ru |...|http://0-50.ru/ne...|0-50.ru|     2017|             3|\n",
      "|Новости 0-50.ru |...|http://0-50.ru/ne...|0-50.ru|      2:0|             2|\n",
      "|Новости 0-50.ru |...|http://0-50.ru/ne...|0-50.ru|       22|             2|\n",
      "|Новости 0-50.ru |...|http://0-50.ru/ne...|0-50.ru|    -2114|             2|\n",
      "|Новости 0-50.ru |...|http://0-50.ru/ne...|0-50.ru|      0:2|             2|\n",
      "|Новости 0-50.ru |...|http://0-50.ru/ne...|0-50.ru|        5|             2|\n",
      "|Новости 0-50.ru |...|http://0-50.ru/ne...|0-50.ru|      2:1|             2|\n",
      "|Новости 0-50.ru |...|http://0-50.ru/ne...|0-50.ru|    -2012|             2|\n",
      "|Новости 0-50.ru |...|http://0-50.ru/ne...|0-50.ru|      0:0|             2|\n",
      "|Новости 0-50.ru |...|http://0-50.ru/ne...|0-50.ru|        7|             2|\n",
      "|Новости 0-50.ru |...|http://0-50.ru/ne...|0-50.ru|      rss|             2|\n",
      "|Новости 0-50.ru |...|http://0-50.ru/ne...|0-50.ru|     2014|             1|\n",
      "|Новости 0-50.ru |...|http://0-50.ru/ne...|0-50.ru|       45|             1|\n",
      "|Новости 0-50.ru |...|http://0-50.ru/ne...|0-50.ru|1@0-50.ru|             1|\n",
      "+--------------------+--------------------+-------+---------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "keywordsExpandedDF = [text: string, uri: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[text: string, uri: string ... 3 more fields]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// create one list with all keywords\n",
    "// keywords distinct = 2017741 (.distinct.count)\n",
    "// keywords total = 15 000 000\n",
    "// total_nr_records = 39653\n",
    "val keywordsExpandedDF = keywordDF\n",
    "                            .withColumn(\"keywords\", explode(($\"keywords\")))\n",
    "                            .withColumn(\"keyword\", $\"keywords._1\")\n",
    "                            .withColumn(\"keyword_weight\", $\"keywords._2\")\n",
    "                            .drop(\"keywords\")\n",
    "\n",
    "keywordsExpandedDF.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reverseIndex = [keyword: string, uri_list: array<string> ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[keyword: string, uri_list: array<string> ... 1 more field]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var reverseIndex = keywordsExpandedDF\n",
    "                        .groupBy($\"keyword\")\n",
    "                        .agg(\n",
    "                            collect_list($\"uri\").alias(\"uri_list\"), \n",
    "                            collect_list($\"keyword_weight\").alias(\"keyword_weights\")\n",
    "                            ) \n",
    "// We can probably collect as struct collect_list(struct($\"uri\", $\"keyword_weight\")).as(\"set\")\n",
    "// This might however lead to problems: https://stackoverflow.com/questions/31864744/spark-dataframes-groupby-into-list\n",
    "                        \n",
    "// Showing here takes 1 minute\n",
    "// reducedKeywords.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:26: error: object cassandra is not a member of package org.apache.spark.sql\n",
       "       import org.apache.spark.sql.cassandra._\n",
       "                                   ^\n",
       "<console>:27: error: object datastax is not a member of package com\n",
       "       import com.datastax.spark.connector._\n",
       "                  ^\n",
       "<console>:28: error: object datastax is not a member of package com\n",
       "       import com.datastax.driver.core.{Session, Cluster, Host, Metadata}\n",
       "                  ^\n",
       "<console>:37: error: not found: value Cluster\n",
       "       val cluster = Cluster.builder().addContactPoint(\"127.0.0.1\").build()\n",
       "                     ^\n",
       "<console>:29: error: object datastax is not a member of package com\n",
       "       import com.datastax.spark.connector.cql.CassandraConnectorConf\n",
       "                  ^\n",
       "<console>:30: error: object datastax is not a member of package com\n",
       "       import com.datastax.spark.connector.rdd.ReadConf\n",
       "                  ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Import and connect to cassandra\n",
    "import org.apache.spark.sql.cassandra._\n",
    "import com.datastax.spark.connector._\n",
    "import com.datastax.driver.core.{Session, Cluster, Host, Metadata}\n",
    "import com.datastax.spark.connector.cql.CassandraConnectorConf\n",
    "import com.datastax.spark.connector.rdd.ReadConf\n",
    "\n",
    "val cluster = Cluster.builder().addContactPoint(\"127.0.0.1\").build()\n",
    "val session = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: java.lang.NoClassDefFoundError\n",
       "Message: Could not initialize class org.apache.spark.sql.cassandra.DefaultSource$\n",
       "StackTrace:   at org.apache.spark.sql.cassandra.package$DataFrameWriterWrapper$.cassandraFormat$extension1(package.scala:55)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Saving to cassandra. This was too complicated with dependencies\n",
    "// in the notebook\n",
    "\n",
    "// connect to Cassandra and make a keyspace and table as explained in the document\n",
    "// reverseIndex.rdd.saveToCassandra(\"search\", \"keywords\",  SomeColumns(\"keyword\", \"links\", \"occurences\"))\n",
    "// reverseIndex.rdd.bulkSaveToCassandra(\"search\", \"keywords\")\n",
    "// Create tables\n",
    "session.execute(\"CREATE KEYSPACE IF NOT EXISTS search WITH REPLICATION =\" +\n",
    "                \"{'class': 'SimpleStrategy', 'replication_factor': 1};\")\n",
    "session.execute(\"CREATE TABLE IF NOT EXISTS search.keywords (keyword text PRIMARY KEY, links list<text>, occurences list<int>);\")\n",
    "\n",
    "reverseIndex\n",
    "    .write\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\n",
    "    .cassandraFormat(\"keywords\", \"search\")\n",
    "    .save()\n",
    "\n",
    "\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
